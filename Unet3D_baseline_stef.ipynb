{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install all dependencies for sgementation-models-3D library.\n",
        "# We will use this library to call 3D unet.\n",
        "# Alternative, you can define your own Unet, if you have skills!\n",
        "!pip install classification-models-3D\n",
        "!pip install efficientnet-3D\n",
        "!pip install segmentation-models-3D\n",
        "\n",
        "# Use patchify to break large volumes into smaller for training \n",
        "#and also to put patches back together after prediction.\n",
        "!pip install patchify"
      ],
      "metadata": {
        "id": "-GgiwGIhyFD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYBoXnNWD8-G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import logging\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "# funky librairies for segmentation\n",
        "import segmentation_models_3D as sm\n",
        "from patchify import patchify, unpatchify\n",
        "\n",
        "print('All librairies sucessfully imported.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz38BGA4DeRk"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "PATH_COLAB = '/content/drive/MyDrive/6_aneurysm_segmentation/challenge_dataset.zip'\n",
        "PATH_DEVICE = './challenge_dataset/'\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    logging.info('Working on Colab.')\n",
        "    \n",
        "    # connect your drive to the session\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/drive/MyDrive/6_aneurysm_segmentation/\n",
        "\n",
        "    # unzip data into the colab session\n",
        "    ! unzip $PATH_COLAB -d /content\n",
        "    logging.info('Data unziped in your Drive.')\n",
        "\n",
        "except:\n",
        "    logging.info('Working on your device.')\n",
        "    \n",
        "    data_exists = os.path.exists(PATH_DEVICE)\n",
        "    \n",
        "    if data_exists:\n",
        "        logging.info(f\"Dataset found on device at : '{PATH_DEVICE}.'\") \n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Data folder not found at '{PATH_DEVICE}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data"
      ],
      "metadata": {
        "id": "X-IV7SfcsNrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_DATASET='./challenge_dataset/'\n",
        "TEST_SIZE = 0.2 # % of test samples from the full dataset\n",
        "VAL_SPLIT = 0.2 # % of training samples kept for the validation metrics\n",
        "CROP = 64"
      ],
      "metadata": {
        "id": "snVgFwtKsNBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get file names\n",
        "file_names = os.listdir(PATH_DATASET)\n",
        "N = len(file_names)\n",
        "print(f'{N} samples in dataset.')\n",
        "\n",
        "# open all .h5 files, split inputs and target masks, store all in np.arrays\n",
        "raw_data = []\n",
        "labels = []\n",
        "names = []\n",
        "\n",
        "for file_name in tqdm(file_names):\n",
        "    f = h5py.File(f'{PATH_DATASET}/{file_name}', 'r')\n",
        "\n",
        "    X, Y = np.array(f['raw']), np.array(f['label'])\n",
        "\n",
        "    X = X[:,CROP:2*CROP,CROP:2*CROP]\n",
        "    Y = Y[:,CROP:2*CROP,CROP:2*CROP]\n",
        "\n",
        "    raw_data.append(X)\n",
        "    labels.append(Y)\n",
        "    names.append(file_name)\n",
        "\n",
        "    # TO KEEP FOR LATER - USEFUL TO ADD FREE SAMPLES BY CROPPING\n",
        "    # X_patches = patchify(X, (64, 64, 64), step=64)  # Step=64 for 64 patches means no overlap\n",
        "    # X_patches_resh = np.reshape(X_patches, (-1, X_patches.shape[3], X_patches.shape[4], X_patches.shape[5]))\n",
        "    # Y_patches = patchify(Y, (64, 64, 64), step=64)  # Step=64 for 64 patches means no overlap\n",
        "    # Y_patches_resh = np.reshape(Y_patches, (-1, Y_patches.shape[3], Y_patches.shape[4], Y_patches.shape[5]))\n",
        "    # raw_data.append(X_patches_resh)\n",
        "    # labels.append(Y_patches_resh)\n",
        "    # names.append(file_name)\n",
        "\n",
        "# convert to arrays for patchify\n",
        "raw_data = np.array(raw_data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# raw_data = np.reshape(raw_data, (-1, raw_data.shape[2], raw_data.shape[3], raw_data.shape[4]))\n",
        "# labels = np.reshape(labels, (-1, labels.shape[2], labels.shape[3], labels.shape[4]))\n",
        "\n",
        "raw_data = np.stack((raw_data,) * 3, axis=-1)\n",
        "labels = np.expand_dims(labels, axis=4)\n",
        "\n",
        "# check shapes\n",
        "print(raw_data.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "0u_AyZ73sksB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCAN_ID = 45\n",
        "DEPTH = 32\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(raw_data[SCAN_ID,:,:,DEPTH,0])\n",
        "ax[1].imshow(labels[SCAN_ID,:,:,DEPTH,0]) # last 0 to get a 2D image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WQPWex6Ouoeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(raw_data, labels, test_size=TEST_SIZE)\n",
        "\n",
        "# train_test_split returns lists, we want arrays for easier calls\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train, dtype='float32')\n",
        "y_test = np.array(y_test, dtype='float32')\n",
        "\n",
        "# check shapes\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "Cmns6kSV1aHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "Gs3sOxQ_w61o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function and coefficients to be used during training:\n",
        "def custom_iou(smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Returns a IoU function, with a custom smoothing parameter.\n",
        "    Such a double function is needed because loss function in Keras are expected\n",
        "    to take only two parameters. Therefore, smooth couldn't be a parameter.\n",
        "    May be removed in future commits beacuse seems finally useless.\n",
        "    \"\"\"\n",
        "    def IoULoss(targets, inputs):\n",
        "        \"\"\"\n",
        "        Returns the intersection over union (IoU) of the two inputs masks.\n",
        "        \"\"\"\n",
        "        inputs = tf.cast(inputs, tf.float32)\n",
        "        targets = tf.cast(targets, tf.float32)\n",
        "        # flatten label and prediction tensors\n",
        "        inputs = K.flatten(inputs)\n",
        "        targets = K.flatten(targets)\n",
        "\n",
        "        intersection = K.sum(targets * inputs)\n",
        "        total = K.sum(targets) + K.sum(inputs)\n",
        "        union = total - intersection\n",
        "        \n",
        "        IoU = (intersection + smooth) / (union + smooth)\n",
        "        return 1 - IoU\n",
        "\n",
        "    return IoULoss"
      ],
      "metadata": {
        "id": "6DdxfpVyxkgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backbones: ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'seresnet18', 'seresnet34', 'seresnet50', 'seresnet101', 'seresnet152', 'seresnext50', 'seresnext101', 'senet154', 'resnext50', 'resnext101', 'vgg16', 'vgg19', 'densenet121', 'densenet169', 'densenet201', 'inceptionresnetv2', 'inceptionv3', 'mobilenet', 'mobilenetv2', 'efficientnetb0', 'efficientnetb1', 'efficientnetb2', 'efficientnetb3', 'efficientnetb4', 'efficientnetb5', 'efficientnetb6', 'efficientnetb7']\n",
        "\n",
        "So far, what worked best for our dataset (IOU_test = 43%) is:\n",
        "```\n",
        "BACKBONE = 'resnet50'\n",
        "LOSS_TYPE = 'jaccard'\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-4\n",
        "```"
      ],
      "metadata": {
        "id": "bkVlkakCxyVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL PARAMETERS\n",
        "encoder_weights = 'imagenet' # Try 'imagenet' or None (random initialization)\n",
        "BACKBONE = 'resnet50'  # Try vgg16, efficientnetb7, inceptionv3, resnet50\n",
        "activation = 'sigmoid' # final layer activation function, sigmoid for binary\n",
        "patch_size = 64 # cube side length\n",
        "n_classes = 1 # num channels output, here binary segmentation so 1\n",
        "channels = 3 # num channels input, need 3 because backbones are trained on RGB\n",
        "LOSS_TYPE = 'jaccard_focal' # check dict 'losses' down below\n",
        "\n",
        "# TRAINING PARAMETERS\n",
        "LR = 1e-4 # starting learning rate\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 12\n",
        "\n",
        "MODEL_NAME = f'./3D_model_{BACKBONE}_{encoder_weights}weights_{EPOCHS}epochs_{LOSS_TYPE}'\n",
        "print(MODEL_NAME)"
      ],
      "metadata": {
        "id": "rBFvmGbwFSzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "optim = tf.keras.optimizers.Adam(LR)\n",
        "\n",
        "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
        "# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
        "dice_loss = sm.losses.DiceLoss() \n",
        "jaccard_loss = sm.losses.JaccardLoss()\n",
        "focal_loss = sm.losses.BinaryFocalLoss()\n",
        "\n",
        "losses = {'dice': dice_loss,\n",
        "          'jaccard': jaccard_loss,\n",
        "          'custom_jaccard': custom_iou(),\n",
        "          'focal_loss': focal_loss,\n",
        "          'dice_focal': dice_loss + (1 * focal_loss), \n",
        "          'jaccard_focal': jaccard_loss + (1 * focal_loss), \n",
        "          }\n",
        "\n",
        "total_loss = losses.get(LOSS_TYPE)\n",
        "assert total_loss is not None, ('Loss not defined. Check your spelling of LOSS_TYPE or the dict losses.')\n",
        "\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
        "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]"
      ],
      "metadata": {
        "id": "FCgohzgAxvZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add callbacks to monitor training \n",
        "weight_path = \"{}_weights.best.hdf5\".format(MODEL_NAME)\n",
        "\n",
        "checkpoint = ModelCheckpoint(weight_path, \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode='min', \n",
        "                             save_weights_only=True)\n",
        "\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', \n",
        "                                   factor=0.5, \n",
        "                                   patience=4, \n",
        "                                   verbose=1, \n",
        "                                   mode='auto', \n",
        "                                   min_delta=0.0001, \n",
        "                                   cooldown=5, \n",
        "                                   min_lr=1e-6)\n",
        "\n",
        "early = EarlyStopping(monitor=\"val_loss\", \n",
        "                      mode=\"min\", \n",
        "                      patience=12) \n",
        "\n",
        "callbacks_list = [checkpoint, \n",
        "                  early, \n",
        "                  reduceLROnPlat]"
      ],
      "metadata": {
        "id": "8SHby8ZSA7di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess input data - otherwise you end up with garbage resutls \n",
        "# and potentially model that does not converge.\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "\n",
        "X_train_prep = preprocess_input(X_train)\n",
        "X_test_prep = preprocess_input(X_test)"
      ],
      "metadata": {
        "id": "1AsL9m0Vylhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model. Here we use Unet but we can also use other model architectures from the library.\n",
        "model = sm.Unet(BACKBONE, classes=n_classes, \n",
        "                input_shape=(patch_size, patch_size, patch_size, channels), \n",
        "                encoder_weights=encoder_weights,\n",
        "                activation=activation)\n",
        "\n",
        "model.compile(optimizer = optim, loss=total_loss, metrics=metrics)\n",
        "# print(model.summary())"
      ],
      "metadata": {
        "id": "c4BU9vulysVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = model.fit(X_train_prep, \n",
        "                    y_train,\n",
        "                    batch_size=BATCH_SIZE, \n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=1,\n",
        "                    validation_split=VAL_SPLIT,\n",
        "                    callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "EatzsLOJyyQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training and validation IoU and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['iou_score']\n",
        "val_acc = history.history['val_iou_score']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
        "plt.title('Training and validation IOU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IOU')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8JT542kX4Aec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "9iHYhf7cMuft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model for testing and predictions. \n",
        "# you need a model instance before loading weights\n",
        "print(f\"Reload weights from : {MODEL_NAME}_weights.best.hdf5\")\n",
        "model.load_weights(f\"{MODEL_NAME}_weights.best.hdf5\")"
      ],
      "metadata": {
        "id": "O4FCXAqL4N2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test_prep)"
      ],
      "metadata": {
        "id": "GC_3XiLr4UUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 0.5\n",
        "\n",
        "y_pred01 = (y_pred > THRESHOLD).astype(int) # float => boolean => binary (0/1)\n",
        "\n",
        "print(f'------ AFTER THRESHOLDING AT {THRESHOLD} ------')\n",
        "print('> sm.metrics.IOUScore :', sm.metrics.IOUScore()(y_test, y_pred01).numpy())\n",
        "print('> Custom IoU :', 1 - custom_iou()(y_test, y_pred01).numpy()) # to check my custom function, it was a loss so 1 - loss\n",
        "\n",
        "# precision_recall_fscore_support report\n",
        "precision, recall, fscore, support = precision_recall_fscore_support(y_test.flatten(), \n",
        "                                                                  y_pred01.flatten()) \n",
        "print('> Precision :', precision[1])\n",
        "print('> Recall :', recall[1])\n",
        "print('> Fscore :', fscore[1])\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test.flatten(), \n",
        "                      y_pred01.flatten())\n",
        "print('\\nConfusion matrix :\\n', cm)"
      ],
      "metadata": {
        "id": "esjwzfGgMsYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test some random images\n",
        "\n",
        "# pick a random test scan and its ground truth mask\n",
        "test_img_number = random.randint(0, len(X_test)-1)\n",
        "print(f'I choose test image n° {test_img_number} ...')\n",
        "\n",
        "test_img = X_test[test_img_number]\n",
        "ground_truth = y_test[test_img_number]\n",
        "\n",
        "# process input image before prediction\n",
        "test_img_input = np.expand_dims(test_img, 0)\n",
        "test_img_input1 = preprocess_input(test_img_input)\n",
        "\n",
        "# prediction\n",
        "test_pred = model.predict(test_img_input1)\n",
        "test_pred = test_pred.squeeze()\n",
        "\n",
        "# thresholding + reshaping\n",
        "print(test_pred.shape)"
      ],
      "metadata": {
        "id": "lmFcNaaV4ikj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot individual slices from test predictions for verification\n",
        "SLICE_MIN, SLICE_MAX = 25, 45\n",
        "\n",
        "for slice in range(SLICE_MIN, SLICE_MAX+1):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(231)\n",
        "    plt.title(f'Testing Image {slice}')\n",
        "    plt.imshow(test_img[slice,:,:,0], cmap='gray')\n",
        "    plt.subplot(232)\n",
        "    plt.title(f'Testing Label {slice}')\n",
        "    plt.imshow(ground_truth[slice,:,:,0])\n",
        "    plt.subplot(233)\n",
        "    plt.title(f'Prediction on test image {slice}')\n",
        "    plt.imshow(test_pred[slice,:,:])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mhxAdxYk4yxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_oWhdfePYHaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "lm2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "ef3dd97f72c7de1b6cf4acbe3ecc775b9ffc9904b79c9969b32a495aa0a05a59"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}